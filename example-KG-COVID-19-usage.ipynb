{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running KG-COVID-19 pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KG-COVID-19 pipeline can be run on the command line or via this notebook. The goal here is to run the pipeline end-to-end.\n",
    "\n",
    "We will also demonstrates some ways that you can use the KG downstream, and show some other features of the framework.\n",
    "\n",
    "**Note:** This notebook assumes that you have already installed the required dependencies for KG-COVID-19. For more information refer to [Installation instructions](https://github.com/Knowledge-Graph-Hub/kg-covid-19/wiki#installation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading all required datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we start with downloading all required datasets as listed in [download.yaml](../download.yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jtr4v/PycharmProjects/kg-covid-19/venv/lib/python3.7/site-packages/biolinkml/__init__.py:158: UserWarning: Some URL processing will fail with python 3.7.5 or earlier.  Current version: sys.version_info(major=3, minor=7, micro=3, releaselevel='final', serial=0)\r\n",
      "  warn(f\"Some URL processing will fail with python 3.7.5 or earlier.  Current version: {sys.version_info}\")\r\n",
      "Downloading files: 100%|█████████████████████| 24/24 [00:00<00:00, 30265.57it/s]\r\n"
     ]
    }
   ],
   "source": [
    "!python run.py download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform all required datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then transform all the datasets and generate the files `nodes.tsv` and `edges.tsv` for each dataset.\n",
    "\n",
    "The files are located in `data/transformed/SOURCE_NAME` where `SOURCE_NAME` is the name of the data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jtr4v/PycharmProjects/kg-covid-19/venv/lib/python3.7/site-packages/biolinkml/__init__.py:158: UserWarning: Some URL processing will fail with python 3.7.5 or earlier.  Current version: sys.version_info(major=3, minor=7, micro=3, releaselevel='final', serial=0)\r\n",
      "  warn(f\"Some URL processing will fail with python 3.7.5 or earlier.  Current version: {sys.version_info}\")\r\n",
      " 58%|█████████████████████▋               | 31664/54137 [08:54<06:01, 62.11it/s]"
     ]
    }
   ],
   "source": [
    "!python run.py transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all datasets into a single graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create a merged graph by reading in the individual nodes.tsv and edges.tsv and merging them.\n",
    "The merge process is driven by the [merge.yaml](../merge.yaml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!python run.py merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The merged graph should be available in `data/merged/` folder.\n",
    "\n",
    "This pipeline generates a graph in KGX TSV format here:\n",
    "`data/merged/merged-kg.tar.gz`\n",
    "Prebuilt graphs are also available here:\n",
    "https://kg-hub.berkeleybop.io/kg-covid-19/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other tooling/functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make training data for machine learning use case\n",
    "\n",
    "KG-COVID-19 contains tooling to produce training data for machine learning. Briefly, a training graph is produced with 80% (by default, override with `-t` parameter) of edges. 20% of edges are removed such that they do not create new components. These graphs are emitted as KGX TSV files in `data/holdouts`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### untar and gunzip the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!tar -xvzf data/merged/merged-kg.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create the training/holdout data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python run.py holdouts -e merged-kg_edges.tsv -n merged-kg_nodes.tsv  # this might take 10 minutes or so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's get some stats on our training graph. We're tightly integrated with ensmallen_graph, so we'll use that package to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from ensmallen_graph import EnsmallenGraph\n",
    "\n",
    "training = EnsmallenGraph.from_csv(\n",
    "    edge_path=\"data/holdouts/pos_train_edges.tsv\",\n",
    "    sources_column='subject',\n",
    "    destinations_column='object',\n",
    "    directed=False,\n",
    "    edge_types_column='edge_label',\n",
    "    default_edge_type='biolink:Association',\n",
    "    node_path=\"data/holdouts/pos_train_nodes.tsv\",\n",
    "    nodes_column='id',\n",
    "    default_node_type='biolink:NamedThing',\n",
    "    node_types_column='category',\n",
    "    ignore_duplicated_edges=True,\n",
    "    ignore_duplicated_nodes=True,\n",
    ");\n",
    "\n",
    "training.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "graph = EnsmallenGraph.from_csv(\n",
    "    edge_path=\"merged-kg_edges.tsv\",\n",
    "    sources_column='subject',\n",
    "    destinations_column='object',\n",
    "    directed=False,\n",
    "    edge_types_column='edge_label',\n",
    "    default_edge_type='biolink:Association',\n",
    "    node_path=\"merged-kg_nodes.tsv\",\n",
    "    nodes_column='id',\n",
    "    default_node_type='biolink:NamedThing',\n",
    "    node_types_column='category',\n",
    "    ignore_duplicated_edges=True,\n",
    "    ignore_duplicated_nodes=True,\n",
    "    force_conversion_to_undirected=True # deprecated, removed in ensmallen_graph 0.4\n",
    ");\n",
    "graph.report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See [these](https://github.com/monarch-initiative/embiggen/blob/master/notebooks/) notebook to generate embeddings from the KG you've created above. There are notebooks to make embeddings using:\n",
    "- [Skipgram](https://github.com/monarch-initiative/embiggen/blob/master/notebooks/Graph%20embedding%20using%20SkipGram.ipynb)\n",
    "- [CBOW](https://github.com/monarch-initiative/embiggen/blob/master/notebooks/Graph%20embedding%20using%20CBOW.ipynb)\n",
    "- [GloVe](https://github.com/monarch-initiative/embiggen/blob/master/notebooks/Graph%20embedding%20using%20GloVe.ipynb)\n",
    "\n",
    "#### These embeddings can then be used to train MLP, random forest, decision tree, and logistic regression classifiers using [this notebook](https://github.com/monarch-initiative/embiggen/blob/master/notebooks/Classical%20Link%20Prediction.ipynb).\n",
    "\n",
    "##### Note: consider running the code in these notebooks on a server with GPUs in order to complete in a reasonable amount of time. Currently on a server with 2 V100 GPUs, the creation of embeddings and the training of classifiers both take on the order of 1 day each to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use prebuilt SPARQL queries to query our Blazegraph endpoint on the commandline\n",
    "\n",
    "KG-COVID-19 has tooling to query our Blazegraph endpoint using predetermined SPARQL queries, and emit the results as a TSV file. Different SPARQL queries on our endpoint or other endpoints can be used by creating a new YAML file and specifying this file with the `-y` flag.\n",
    "\n",
    "The following is a simple query that retrieves a summary of the types of entities in the current KG-COVID-19 knowledge graph loaded on Blazegraph endpoint. These are counted as Biolink categories, which are high level entities such as genes, proteins, publications, etc. You can read more about the Biolink data model [here](https://biolink.github.io/biolink-model/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!python run.py query -y queries/sparql/query-01-bl-cat-counts.yaml # or make a new YAML file and write your own query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('data/queries/query-01-bl-cat-counts.tsv', newline='') as tsv:\n",
    "    read_tsv = csv.reader(tsv, delimiter=\"\\t\")\n",
    "    for row in read_tsv:\n",
    "      print(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}